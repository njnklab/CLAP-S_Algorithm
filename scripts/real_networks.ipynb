{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/home/a001/Documents/ZhengHaoyu/python/1_important/OCMN')\n",
    "\n",
    "import glob\n",
    "import networkx as nx\n",
    "from typing import Dict, List, Tuple, Set\n",
    "import time\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "import config\n",
    "from utils.utils import save_network, setup_logger, create_output_file\n",
    "from matching import Matching, MultiMatching\n",
    "\n",
    "logger = setup_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_network_stats(network_dir: str) -> None:\n",
    "    \"\"\"分析网络的基本统计信息，包括层数、节点数和边数\"\"\"\n",
    "    # 获取网络名称\n",
    "    network_name = os.path.basename(network_dir)\n",
    "    print(f\"分析网络: {network_name}\")\n",
    "    \n",
    "    # 查找layers文件\n",
    "    layers_file = glob.glob(os.path.join(network_dir, \"Dataset\", \"*_layers.txt\"))[0]\n",
    "    edges_file = glob.glob(os.path.join(network_dir, \"Dataset\", \"*_multiplex.edges\"))[0]\n",
    "    \n",
    "    # 读取层信息\n",
    "    layers = {}\n",
    "    with open(layers_file, 'r', encoding='utf-8') as f:\n",
    "        next(f)  # 跳过headers行\n",
    "        for line in f:\n",
    "            layer_id, layer_name = line.strip().split()\n",
    "            layers[layer_id] = {'name': layer_name, 'nodes': set(), 'edges': 0}\n",
    "    \n",
    "    # 读取边信息\n",
    "    total_nodes = set()\n",
    "    with open(edges_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            layer_id, node1, node2, weight = line.strip().split()\n",
    "            layers[layer_id]['nodes'].add(node1)\n",
    "            layers[layer_id]['nodes'].add(node2)\n",
    "            layers[layer_id]['edges'] += 1\n",
    "            total_nodes.add(node1)\n",
    "            total_nodes.add(node2)\n",
    "    \n",
    "    # 输出统计信息\n",
    "    print(f\"网络层数: {len(layers)}\")\n",
    "    print(f\"总节点数: {len(total_nodes)}\")\n",
    "    total_edges = sum(layer['edges'] for layer in layers.values())\n",
    "    print(f\"总边数: {total_edges}\")\n",
    "    print(\"各层统计信息:\")\n",
    "    print(\"层ID\\t层名称\\t\\t节点数\\t边数\")\n",
    "    for layer_id, info in layers.items():\n",
    "        print(f\"{layer_id}\\t{info['name']}\\t\\t{len(info['nodes'])}\\t{info['edges']}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "for network_dir in sorted(glob.glob(os.path.join(config.REAL_NET_PATH, \"*\"))):\n",
    "    if os.path.isdir(network_dir):\n",
    "        try:\n",
    "            analyze_network_stats(network_dir)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"分析网络 {os.path.basename(network_dir)} 统计信息时出错: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# “Structural reducibility of multilayer networks”\n",
    "# M. De Domenico, V. Nicosia, A. Arenas, and V. Latora\n",
    "# Nature Communications 2015 6, 6864\n",
    "\n",
    "NETWORKS_TO_PROCESS = {\n",
    "    # 生物\n",
    "    # Ref: C. Stark, B. -J. Breitkreutz, T. Reguly, L. Boucher, A. Breitkreutz, and M. Tyers. - \"Biogrid: a general repository for interaction datasets\" - Nucleic Acids Research 2006 34 (1) D535–D539\n",
    "    \"Arabidopsis\": [[\"direct_interaction\", \"physical_association\"]],\n",
    "    \"Celegans\": [[\"direct_interaction\", \"physical_association\"]],\n",
    "    \"Drosophila\": [[\"direct_interaction\", \"physical_association\"]],\n",
    "    \"HumanHIV1\": [[\"direct_interaction\", \"physical_association\"]],\n",
    "    \"SacchPomb\": [[\"direct_interaction\", \"physical_association\"]],\n",
    "    \"Rattus\": [[\"direct_interaction\", \"physical_association\"]],\n",
    "    # Ref: Beth L. Chen, David H. Hall, and Dmitri B. Chklovskii - \"Wiring optimization can relate neuronal structure and function\" - PNAS 2006 103 (12) 4723–4728\n",
    "    \"CelegansConnectome\": [[\"ElectrJ\", \"MonoSyn\"]],\n",
    "    # Ref: M. Costanzo et al. - \"The Genetic Landscape of a Cell\" - Science 2010 327 (5964) 425-431\n",
    "    \"YeastLandscape\": [[\"positive_interactions\", \"negative_interactions\"]],\n",
    "    # 社交\n",
    "    # Ref: E. Omodei, M. De Domenico, A. Arenas. - Characterizing interactions in online social networks during exceptional events.. Front. Phys. 3, 59 (2015)\n",
    "    \"Cannes\": [[\"RT\", \"MT\"]],\n",
    "    \"MLKing\": [[\"RT\", \"MT\"]],\n",
    "    \"MoscowAthletics\": [[\"RT\", \"MT\"]],\n",
    "    \"NYClimate\": [[\"RT\", \"MT\"]],\n",
    "    # Ref: M. De Domenico, E. G. Altmann. - Unraveling the Origin of Social Bursts in Collective Attention.. Scientific Reports 10, 4629 (2020)\n",
    "    \"NBAFinals\": [[\"RT\", \"MT\"]],\n",
    "    \"Sanremo\": [[\"RT\", \"MT\"]],\n",
    "    \"UCLFinal\": [[\"RT\", \"MT\"]],\n",
    "    \"GravitationalWaves\": [[\"RT\", \"MT\"]],\n",
    "    # 人际关系\n",
    "    # Ref: D. Krackhardt - \"Cognitive social structures\". Social Networks (1987), 9, 104-134\n",
    "    \"KrackhardtHighTech\": [\n",
    "        [\"friendship\", \"advice\"],\n",
    "        [\"friendship\", \"Reports_to\"],\n",
    "    ],\n",
    "    # Ref: Emmanuel Lazega - \"The Collegial Phenomenon: The Social Mechanisms of Cooperation Among Peers in a Corporate Law Partnership\". Oxford University Press (2001)\n",
    "    \"LazegaLawFirm\": [\n",
    "        [\"friendship\", \"advice\"],\n",
    "        [\"friendship\", \"co-work\"],\n",
    "    ],\n",
    "    # Ref: J. Coleman, E. Katz, and H. Menzel.- \"The Diffusion of an Innovation Among Physicians\". Sociometry (1957) 20:253-270.\n",
    "    \"PhysiciansInnovation\": [\n",
    "        [\"friendship\", \"advice\"],\n",
    "        [\"friendship\", \"discussion\"],\n",
    "    ],\n",
    "    # \"VickersChan7thGraders\": [\n",
    "    #     [\"best_friends\", \"get_on_with\"],\n",
    "    #     [\"best_friends\", \"work_with\"]\n",
    "    # ],\n",
    "    # \"KapfererTailorShop\": [\n",
    "    #     [\"TS1\", \"TS2\"], \n",
    "    #     [\"TI1\", \"TI2\"],\n",
    "    #     [\"TS1\", \"TI1\"],\n",
    "    #     [\"TS2\", \"TI2\"],\n",
    "    # ],\n",
    "    # 交通\n",
    "    # \"EUAirMultiplexTransport\": [\n",
    "    #     # german\n",
    "    #     [\"Lufthansa\", \"Air_Berlin\"],\n",
    "    #     # uk\n",
    "    #     [\"Easyjet\", \"British_Airways\"],\n",
    "    #     # 爱尔兰\n",
    "    #     [\"Ryanair\", \"Air_Lingus\"],\n",
    "    #     # Spain\n",
    "    #     [\"Iberia\", \"Vueling_Airlines\"],\n",
    "    #     # 荷兰\n",
    "    #     [\"KLM\", \"Transavia_Holland\"],\n",
    "    #     # 比利时\n",
    "    #     [\"TNT_Airways\", \"European_Air_Transport\"],\n",
    "    # ],\n",
    "    # \"LondonTransport\": [\n",
    "    #     [\"Tube\", \"Overground\"],\n",
    "    # ],\n",
    "}\n",
    "\n",
    "UNDIRECTED_NETWORKS = [\n",
    "    \"EUAirMultiplexTransport\",\n",
    "    \"LondonTransport\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_layers_info(network_dir: str) -> Dict[str, str]:\n",
    "    \"\"\"读取网络层的信息，返回层ID到层名称的映射\"\"\"\n",
    "    layers_file = glob.glob(os.path.join(network_dir, \"Dataset\", \"*_layers.txt\"))[0]\n",
    "    layers = {}\n",
    "    with open(layers_file, 'r', encoding='utf-8') as f:\n",
    "        next(f)  # 跳过headers行\n",
    "        for line in f:\n",
    "            layer_id, layer_name = line.strip().split()\n",
    "            layers[layer_id] = layer_name\n",
    "    return layers\n",
    "\n",
    "def get_layer_nodes(network_dir: str, layer_ids: List[str]) -> Set[int]:\n",
    "    \"\"\"获取指定层中出现的所有节点\"\"\"\n",
    "    edges_file = glob.glob(os.path.join(network_dir, \"Dataset\", \"*_multiplex.edges\"))[0]\n",
    "    nodes = set()\n",
    "    \n",
    "    node_1 = set()\n",
    "    node_2 = set()\n",
    "    with open(edges_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) != 4:\n",
    "                continue\n",
    "            \n",
    "            lid, node1, node2, _ = parts\n",
    "            if lid in layer_ids:\n",
    "                # 节点编号加1（因为原始数据从0开始）\n",
    "                nodes.add(int(node1) + 1)\n",
    "                nodes.add(int(node2) + 1)\n",
    "                if lid == layer_ids[0]:\n",
    "                    node_1.add(int(node1) + 1)\n",
    "                    node_1.add(int(node2) + 1)\n",
    "                else:\n",
    "                    node_2.add(int(node1) + 1)\n",
    "                    node_2.add(int(node2) + 1)\n",
    "    \n",
    "    return nodes, node_1, node_2\n",
    "\n",
    "def create_layer_graph(network_dir: str, layer_id: str, all_nodes: Set[int]) -> nx.DiGraph:\n",
    "    \"\"\"为指定的层创建有向图\"\"\"\n",
    "    edges_file = glob.glob(os.path.join(network_dir, \"Dataset\", \"*_multiplex.edges\"))[0]\n",
    "    graph = nx.DiGraph()\n",
    "    \n",
    "    # 添加所有节点\n",
    "    for node in all_nodes:\n",
    "        graph.add_node(node)\n",
    "    \n",
    "    edge_count = 0\n",
    "    # 读取指定层的边\n",
    "    with open(edges_file, 'r', encoding='utf-8') as f:\n",
    "        for line_num, line in enumerate(f, 0):\n",
    "            try:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) != 4:\n",
    "                    logger.warning(f\"第 {line_num} 行格式错误: {line.strip()}\")\n",
    "                    continue\n",
    "                    \n",
    "                lid, node1, node2, _ = parts\n",
    "                if lid == layer_id:\n",
    "                    try:\n",
    "                        if os.path.basename(network_dir) in UNDIRECTED_NETWORKS:\n",
    "                            graph.add_edge(int(node1) + 1, int(node2) + 1)\n",
    "                            graph.add_edge(int(node2) + 1, int(node1) + 1)\n",
    "                        else:\n",
    "                            graph.add_edge(int(node1) + 1, int(node2) + 1)\n",
    "                        edge_count += 1\n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"添加边 ({node1}, {node2}) 时出错: {str(e)}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"处理第 {line_num} 行时出错: {line.strip()}, 错误信息: {str(e)}\")\n",
    "    \n",
    "    return graph\n",
    "\n",
    "def save_graph_to_file(graph: nx.DiGraph, net_name: str, layer_name: str):\n",
    "    \"\"\"将图保存为边列表格式\"\"\"\n",
    "    filename = os.path.join(config.TEST_NET_PATH, f\"{net_name}_{layer_name}.txt\")\n",
    "    save_network(graph, filename)\n",
    "\n",
    "def process_network_layers(net_name: str, layer_names: List[str], output_file_name: str):\n",
    "    \"\"\"处理指定网络的两个层，返回intersection和union的变化\"\"\"\n",
    "    network_dir = os.path.join(config.REAL_NET_PATH, net_name)\n",
    "    \n",
    "    # 获取层ID到层名称的映射\n",
    "    layers_info = read_layers_info(network_dir)\n",
    "    \n",
    "    # 找到指定层名称对应的层ID\n",
    "    layer_ids = []\n",
    "    for lid, lname in layers_info.items():\n",
    "        if lname in layer_names:\n",
    "            layer_ids.append(lid)\n",
    "\n",
    "    # 打印层ids\n",
    "    if len(layer_ids) != 2:\n",
    "        raise ValueError(f\"在网络 {net_name} 中未找到指定的两个层: {layer_names}\")\n",
    "    logger.debug(f\"在网络 {net_name} 中的层ID： {layer_ids}\")\n",
    "    \n",
    "    # 获取两层中的所有节点\n",
    "    all_nodes, node_1, node_2 = get_layer_nodes(network_dir, layer_ids)\n",
    "    logger.debug(f\"两层中共有 {len(all_nodes)} 个节点\")\n",
    "    \n",
    "    # 为每一层创建图\n",
    "    graphs = []\n",
    "    for i, lid in enumerate(layer_ids):\n",
    "        if os.path.basename(network_dir) in UNDIRECTED_NETWORKS:\n",
    "            logger.debug(f\"网络 {os.path.basename(network_dir)} 是无向网络\")\n",
    "        \n",
    "        graph = create_layer_graph(network_dir, lid, all_nodes)\n",
    "        # 保存图到文件\n",
    "        save_graph_to_file(graph, net_name, layer_names[i])\n",
    "        graphs.append(graph)\n",
    "\n",
    "    # 打印图的信息\n",
    "    for i, g in enumerate(graphs):\n",
    "        logger.debug(f\"图 {i} 的信息：节点数：{g.number_of_nodes()}, 边数：{g.number_of_edges()}\")\n",
    "    \n",
    "    # 创建Matching对象\n",
    "    matchings = []\n",
    "    for graph in graphs:\n",
    "        matching = Matching(graph)\n",
    "        matching.HK_algorithm()\n",
    "        matching.find_all_alternating_reachable_set()\n",
    "        matchings.append(matching)\n",
    "    \n",
    "    multi_matching = MultiMatching(matchings)\n",
    "    multi_matching_rsuu = copy.deepcopy(multi_matching)\n",
    "    multi_matching_glde = copy.deepcopy(multi_matching)\n",
    "    multi_matching_ilp = copy.deepcopy(multi_matching)\n",
    "\n",
    "    pre_diff_mds_1_size = len(multi_matching.matchings[0].driver_nodes)\n",
    "    pre_diff_mds_2_size = len(multi_matching.matchings[1].driver_nodes)\n",
    "    pre_union_size = len(set.union(multi_matching.matchings[0].driver_nodes, multi_matching.matchings[1].driver_nodes))\n",
    "\n",
    "    # 定义超时装饰器\n",
    "    import signal\n",
    "    class TimeoutException(Exception):\n",
    "        pass\n",
    "    def timeout_handler(signum, frame):\n",
    "        raise TimeoutException()\n",
    "    signal.signal(signal.SIGALRM, timeout_handler)\n",
    "\n",
    "    # CLAPS\n",
    "    try:\n",
    "        signal.alarm(500)\n",
    "        start_time = time.time()\n",
    "        _, _, _, union_size_clap_s, average_depth = multi_matching.CLAPS()\n",
    "        end_time = time.time()\n",
    "        time_clap_s = end_time - start_time\n",
    "        signal.alarm(0)\n",
    "    except TimeoutException:\n",
    "        union_size_clap_s = average_depth = -1\n",
    "        time_clap_s = -1\n",
    "        signal.alarm(0)\n",
    "\n",
    "    # RSU\n",
    "    try:\n",
    "        signal.alarm(500)\n",
    "        start_time = time.time()\n",
    "        union_size_rsu = multi_matching_rsuu.RSU()\n",
    "        end_time = time.time()\n",
    "        time_rsu = end_time - start_time\n",
    "        signal.alarm(0)\n",
    "    except TimeoutException:\n",
    "        union_size_rsu = -1\n",
    "        time_rsu = -1\n",
    "        signal.alarm(0)\n",
    "\n",
    "    # CLAPG\n",
    "    try:\n",
    "        signal.alarm(500)\n",
    "        start_time = time.time()\n",
    "        union_size_clap_g = multi_matching_glde.CLAPG()\n",
    "        end_time = time.time()\n",
    "        time_clap_g = end_time - start_time\n",
    "        signal.alarm(0)\n",
    "    except TimeoutException:\n",
    "        union_size_clap_g = -1\n",
    "        time_clap_g = -1\n",
    "        signal.alarm(0)\n",
    "\n",
    "    # ILP_exact\n",
    "    try:\n",
    "        signal.alarm(500)\n",
    "        start_time = time.time()\n",
    "        union_size_ilp = multi_matching_ilp.ILP_exact(budget_mode=\"auto\")\n",
    "        end_time = time.time()\n",
    "        time_ilp = end_time - start_time\n",
    "        signal.alarm(0)\n",
    "    except TimeoutException:\n",
    "        union_size_ilp = -1\n",
    "        time_ilp = -1\n",
    "        signal.alarm(0)\n",
    "    except ValueError:\n",
    "        union_size_ilp = -1\n",
    "        time_ilp = -1\n",
    "        signal.alarm(0)\n",
    "\n",
    "    with open(output_file_name, \"a\", encoding=\"utf-8\") as output_file:\n",
    "        output_file.write(\",\".join([\n",
    "            f\"{net_name}\", str(layer_names[0]), str(layer_names[1]), \n",
    "            str(len(all_nodes)), str(len(node_1)), str(len(node_2)), \n",
    "            str(len(graphs[0].edges)), str(len(graphs[1].edges)), \n",
    "            str(2 * (len(graphs[0].edges) + len(graphs[1].edges)) / len(all_nodes)), \n",
    "            str(2 * len(graphs[0].edges) / len(node_1)), str(2 * len(graphs[1].edges) / len(node_2)), \n",
    "            str(len(matchings[0].driver_nodes)), str(len(matchings[1].driver_nodes)),\n",
    "            str(pre_diff_mds_1_size), str(pre_diff_mds_2_size), str(pre_union_size), \n",
    "            str(union_size_clap_s), str(union_size_rsu), str(union_size_clap_g), str(union_size_ilp), \n",
    "            str(average_depth),\n",
    "            str(time_clap_s),str(time_rsu), str(time_clap_g), str(time_ilp)\n",
    "        ]) + \"\\n\")\n",
    "\n",
    "    # N, N_1, N_2, <k>, <k_1>, <k_2>\n",
    "    return str(len(all_nodes)), str(len(node_1)), str(len(node_2)), str(2 * (len(graphs[0].edges) + len(graphs[1].edges)) / len(all_nodes)), str(2 * len(graphs[0].edges) / len(node_1)), str(2 * len(graphs[1].edges) / len(node_2))\n",
    "\n",
    "def real_networks(\n",
    "    result_columns=[\n",
    "        \"network_name\", \"layer_name_1\", \"layer_name_2\", \n",
    "        \"N\", \"N_1\", \"N_2\", \"E_1\", \"E_2\", \"<k>\", \"<k_1>\", \"<k_2>\", \n",
    "        \"MDS_1\", \"MDS_2\", \n",
    "        \"Diff_MDS_1\", \"Diff_MDS_2\", \"UDS_0\", \n",
    "        \"UDS_CLAPS\", \"UDS_RSU\", \"UDS_CLAPG\", \"UDS_ILP\", \n",
    "        \"clap_average_length\", \n",
    "        \"time_CLAPS\", \"time_RSU\", \"time_CLAPG\", \"time_ILP\"]\n",
    "):\n",
    "    info_df = pd.DataFrame()\n",
    "    output_file_name = create_output_file(result_columns, \"real_networks\")\n",
    "    for net_name, layer_pairs in NETWORKS_TO_PROCESS.items():\n",
    "        for layer_names in layer_pairs:\n",
    "            if len(layer_pairs) > 1:\n",
    "                net_abbr = f\"{net_name}-{layer_names[0][0].lower()}&{layer_names[1][0].lower()}\"\n",
    "            else:\n",
    "                net_abbr = net_name\n",
    "            print(f\"处理网络: {net_name}\")\n",
    "            print(f\"层: {layer_names[0]} - {layer_names[1]}\")\n",
    "            N, N_1, N_2, k, k_1, k_2 = process_network_layers(net_name, layer_names, output_file_name)\n",
    "            info_df = pd.concat([info_df, pd.DataFrame({\n",
    "                \"network_name\": [net_abbr],\n",
    "                \"layer_name_1\": [layer_names[0]],\n",
    "                \"layer_name_2\": [layer_names[1]],\n",
    "                \"N\": [N],\n",
    "                \"N_1\": [N_1],\n",
    "                \"N_2\": [N_2],\n",
    "                \"<k>\": [round(float(k), 2)],\n",
    "                \"<k_1>\": [round(float(k_1), 2)],\n",
    "                \"<k_2>\": [round(float(k_2), 2)]\n",
    "            })], ignore_index=True)\n",
    "            print()\n",
    "        print()\n",
    "    return info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_networks()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
