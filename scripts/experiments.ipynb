{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/home/a001/Documents/ZhengHaoyu/python/1_important/OCMN')\n",
    "import glob\n",
    "from utils.generator import Generator, ERGenerator, BAGenerator, SFGenerator\n",
    "from utils.utils import save_network, read_network, setup_logger, create_output_file\n",
    "from utils.plot import transform_graphs, visualize_networks_with_bipartite, visualize_networks_with_matching_and_bipartite\n",
    "from matching import Matching, MultiMatching\n",
    "import config\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "import copy\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Synthetic Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 81/81 [07:19<00:00,  5.42s/it]\n"
     ]
    }
   ],
   "source": [
    "networks = [\"base\"] + [f\"overlap={overlap}\" for overlap in config.NETWORK_OVERLAP]\n",
    "\n",
    "def generate_and_save_networks(k_range={\"start\": 2, \"end\": 10, \"step\": 0.1}):\n",
    "    for k in tqdm(np.arange(k_range['start'], k_range['end'] + k_range[\"step\"], k_range[\"step\"])):\n",
    "        for n in config.NETWORK_NODES_LIST:\n",
    "            k = round(k, 2)\n",
    "            \n",
    "            er_generator = ERGenerator(n, k)\n",
    "            er_graphs = er_generator.generate_networks(len(networks), config.NETWORK_OVERLAP)\n",
    "            dir = f\"ER_n={n}_k={k}\"\n",
    "            os.makedirs(os.path.join(config.SYNTHETIC_NET_PATH, 'ER', dir), exist_ok=True)\n",
    "            for i in range(len(networks)):\n",
    "                save_network(er_graphs[i], os.path.join(config.SYNTHETIC_NET_PATH, 'ER', dir, f\"{networks[i]}.txt\"))\n",
    "            \n",
    "            ba_generator = SFGenerator(n, k)\n",
    "            ba_graphs = ba_generator.generate_networks(len(networks), config.NETWORK_OVERLAP)\n",
    "            dir = f\"BA_n={n}_k={k}\"\n",
    "            os.makedirs(os.path.join(config.SYNTHETIC_NET_PATH, 'BA', dir), exist_ok=True)\n",
    "            for i in range(len(networks)):\n",
    "                save_network(ba_graphs[i], os.path.join(config.SYNTHETIC_NET_PATH, 'BA', dir, f\"{networks[i]}.txt\"))\n",
    "\n",
    "generate_and_save_networks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 不同网络类型、不同平均度k、不同节点数N、重复实验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing n=1000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 81/81 [14:04<00:00, 10.42s/it]\n",
      "100%|██████████| 81/81 [14:17<00:00, 10.58s/it]\n",
      "100%|██████████| 81/81 [13:43<00:00, 10.17s/it]\n"
     ]
    }
   ],
   "source": [
    "def optimization_amount(\n",
    "    k_range={\"start\": 2, \"end\": 10, \"step\": 0.1},\n",
    "    n_list=config.NETWORK_NODES_LIST,\n",
    "    result_columns=[\"network_type\", \"N\", \"<k>\", \"seq\", \"MDS_1\", \"MDS_2\", \"Diff_MDS_1\", \"Diff_MDS_2\", \"UMDS_0\", \"UMDS_1\", \"UMDS_2\", \"average_depth\", \"time_1\", \"time_2\"]\n",
    "):\n",
    "    output_file_name = create_output_file(result_columns, \"optimization_amount\")\n",
    "\n",
    "    for n in n_list:\n",
    "        print(f\"Processing n={n}...\")\n",
    "    \n",
    "        for network_type in [\"ER\", \"BA\"]:\n",
    "            for k in tqdm(np.arange(k_range['start'], k_range['end'] + k_range[\"step\"], k_range[\"step\"])):\n",
    "                k = round(k, 2)\n",
    "                for seq in range(10):\n",
    "                    dir = f\"{network_type}_n={n}_k={k}\"\n",
    "                    matchings = []\n",
    "                    for file in os.listdir(os.path.join(config.SYNTHETIC_NET_PATH, network_type, dir)):\n",
    "                        # 包含base或-1的文件名\n",
    "                        if \"base\" in file or \"-1\" in file:\n",
    "                            graph = read_network(os.path.join(config.SYNTHETIC_NET_PATH, network_type, dir, file), n)\n",
    "                            matching = Matching(graph)\n",
    "                            matching.HK_algorithm()\n",
    "                            matching.find_all_alternating_reachable_set()\n",
    "                            matchings.append(matching)\n",
    "                    \n",
    "                    multi_matching = MultiMatching(matchings)\n",
    "                    baseline_matching = copy.deepcopy(multi_matching)\n",
    "\n",
    "                    start_time = time.time()\n",
    "                    pre_diff_mds_1_size, pre_diff_mds_2_size, pre_union_size, union_size, average_depth = multi_matching.MOUI()\n",
    "                    end_time = time.time()\n",
    "                    time_2 = end_time - start_time\n",
    "                    \n",
    "                    start_time = time.time()\n",
    "                    min_union_size = baseline_matching.RRMU()\n",
    "                    end_time = time.time()\n",
    "                    time_1 = end_time - start_time\n",
    "                    \n",
    "                    with open(output_file_name, \"a\", encoding=\"utf-8\") as output_file:\n",
    "                        output_file.write(\",\".join([\n",
    "                            f\"{network_type}+{network_type}\", str(n), str(k), str(seq), \n",
    "                            str(len(matchings[0].driver_nodes)), str(len(matchings[1].driver_nodes)), \n",
    "                            str(pre_diff_mds_1_size), str(pre_diff_mds_2_size), \n",
    "                            str(pre_union_size), str(min_union_size), str(union_size), str(average_depth),\n",
    "                            str(time_1), str(time_2)\n",
    "                        ]) + \"\\n\")\n",
    "        \n",
    "        for k in tqdm(np.arange(k_range['start'], k_range['end'] + k_range[\"step\"], k_range[\"step\"])):\n",
    "            k = round(k, 2)\n",
    "            for seq in range(10):\n",
    "                matchings = []\n",
    "                graph_1 = read_network(os.path.join(config.SYNTHETIC_NET_PATH, \"ER\", f\"ER_n={n}_k={k}\", f\"base.txt\"), n)\n",
    "                matching_1 = Matching(graph_1)\n",
    "                matching_1.HK_algorithm()\n",
    "                matching_1.find_all_alternating_reachable_set()\n",
    "                matchings.append(matching_1)\n",
    "                graph_2 = read_network(os.path.join(config.SYNTHETIC_NET_PATH, \"BA\", f\"BA_n={n}_k={k}\", f\"base.txt\"), n)\n",
    "                matching_2 = Matching(graph_2)\n",
    "                matching_2.HK_algorithm()\n",
    "                matching_2.find_all_alternating_reachable_set()\n",
    "                matchings.append(matching_2)\n",
    "\n",
    "                multi_matching = MultiMatching(matchings)\n",
    "                baseline_matching = copy.deepcopy(multi_matching)\n",
    "                \n",
    "                start_time = time.time()\n",
    "                pre_diff_mds_1_size, pre_diff_mds_2_size, pre_union_size, union_size, average_depth = multi_matching.MOUI()\n",
    "                end_time = time.time()\n",
    "                time_2 = end_time - start_time\n",
    "                \n",
    "                start_time = time.time()\n",
    "                min_union_size = baseline_matching.RRMU()\n",
    "                end_time = time.time()\n",
    "                time_1 = end_time - start_time\n",
    "                with open(output_file_name, \"a\", encoding=\"utf-8\") as output_file:\n",
    "                    output_file.write(\",\".join([\n",
    "                        f\"ER+BA\", str(n), str(k), str(seq), \n",
    "                        str(len(matching_1.driver_nodes)), str(len(matching_2.driver_nodes)), \n",
    "                        str(pre_diff_mds_1_size), str(pre_diff_mds_2_size), \n",
    "                        str(pre_union_size), str(min_union_size), str(union_size), str(average_depth),\n",
    "                        str(time_1), str(time_2)\n",
    "                    ]) + \"\\n\")\n",
    "\n",
    "optimization_amount()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 不同网络类型、不同平均度k、不同网络重叠程度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing n=1000...\n",
      "\tProcessing ER...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tOverlap=0.1: 100%|██████████| 81/81 [01:25<00:00,  1.05s/it]\n",
      "\t\tOverlap=0.11: 100%|██████████| 81/81 [01:25<00:00,  1.05s/it]\n",
      "\t\tOverlap=0.12: 100%|██████████| 81/81 [01:24<00:00,  1.05s/it]\n",
      "\t\tOverlap=0.13: 100%|██████████| 81/81 [01:24<00:00,  1.04s/it]\n",
      "\t\tOverlap=0.14: 100%|██████████| 81/81 [01:24<00:00,  1.04s/it]\n",
      "\t\tOverlap=0.15: 100%|██████████| 81/81 [01:26<00:00,  1.06s/it]\n",
      "\t\tOverlap=0.16: 100%|██████████| 81/81 [01:24<00:00,  1.04s/it]\n",
      "\t\tOverlap=0.17: 100%|██████████| 81/81 [01:25<00:00,  1.05s/it]\n",
      "\t\tOverlap=0.18: 100%|██████████| 81/81 [01:24<00:00,  1.04s/it]\n",
      "\t\tOverlap=0.19: 100%|██████████| 81/81 [01:23<00:00,  1.03s/it]\n",
      "\t\tOverlap=0.2: 100%|██████████| 81/81 [01:23<00:00,  1.03s/it]\n",
      "\t\tOverlap=0.21: 100%|██████████| 81/81 [01:24<00:00,  1.04s/it]\n",
      "\t\tOverlap=0.22: 100%|██████████| 81/81 [01:25<00:00,  1.05s/it]\n",
      "\t\tOverlap=0.23: 100%|██████████| 81/81 [01:24<00:00,  1.04s/it]\n",
      "\t\tOverlap=0.24: 100%|██████████| 81/81 [01:24<00:00,  1.05s/it]\n",
      "\t\tOverlap=0.25: 100%|██████████| 81/81 [01:25<00:00,  1.05s/it]\n",
      "\t\tOverlap=0.26: 100%|██████████| 81/81 [01:25<00:00,  1.05s/it]\n",
      "\t\tOverlap=0.27: 100%|██████████| 81/81 [01:24<00:00,  1.04s/it]\n",
      "\t\tOverlap=0.28: 100%|██████████| 81/81 [01:24<00:00,  1.04s/it]\n",
      "\t\tOverlap=0.29: 100%|██████████| 81/81 [01:24<00:00,  1.05s/it]\n",
      "\t\tOverlap=0.3: 100%|██████████| 81/81 [01:23<00:00,  1.04s/it]\n",
      "\t\tOverlap=0.31: 100%|██████████| 81/81 [01:23<00:00,  1.03s/it]\n",
      "\t\tOverlap=0.32: 100%|██████████| 81/81 [01:24<00:00,  1.04s/it]\n",
      "\t\tOverlap=0.33: 100%|██████████| 81/81 [01:24<00:00,  1.05s/it]\n",
      "\t\tOverlap=0.34: 100%|██████████| 81/81 [01:23<00:00,  1.03s/it]\n",
      "\t\tOverlap=0.35: 100%|██████████| 81/81 [01:24<00:00,  1.05s/it]\n",
      "\t\tOverlap=0.36: 100%|██████████| 81/81 [01:23<00:00,  1.04s/it]\n",
      "\t\tOverlap=0.37: 100%|██████████| 81/81 [01:24<00:00,  1.04s/it]\n",
      "\t\tOverlap=0.38: 100%|██████████| 81/81 [01:24<00:00,  1.05s/it]\n",
      "\t\tOverlap=0.39: 100%|██████████| 81/81 [01:23<00:00,  1.03s/it]\n",
      "\t\tOverlap=0.4: 100%|██████████| 81/81 [01:24<00:00,  1.05s/it]\n",
      "\t\tOverlap=0.41: 100%|██████████| 81/81 [01:25<00:00,  1.05s/it]\n",
      "\t\tOverlap=0.42: 100%|██████████| 81/81 [01:24<00:00,  1.05s/it]\n",
      "\t\tOverlap=0.43: 100%|██████████| 81/81 [01:23<00:00,  1.03s/it]\n",
      "\t\tOverlap=0.44: 100%|██████████| 81/81 [01:24<00:00,  1.04s/it]\n",
      "\t\tOverlap=0.45: 100%|██████████| 81/81 [01:23<00:00,  1.04s/it]\n",
      "\t\tOverlap=0.46: 100%|██████████| 81/81 [01:23<00:00,  1.03s/it]\n",
      "\t\tOverlap=0.47: 100%|██████████| 81/81 [01:23<00:00,  1.04s/it]\n",
      "\t\tOverlap=0.48: 100%|██████████| 81/81 [01:25<00:00,  1.05s/it]\n",
      "\t\tOverlap=0.49: 100%|██████████| 81/81 [01:23<00:00,  1.03s/it]\n",
      "\t\tOverlap=0.5: 100%|██████████| 81/81 [01:24<00:00,  1.04s/it]\n",
      "\t\tOverlap=0.51: 100%|██████████| 81/81 [01:24<00:00,  1.04s/it]\n",
      "\t\tOverlap=0.52: 100%|██████████| 81/81 [01:24<00:00,  1.04s/it]\n",
      "\t\tOverlap=0.53: 100%|██████████| 81/81 [01:24<00:00,  1.04s/it]\n",
      "\t\tOverlap=0.54: 100%|██████████| 81/81 [01:24<00:00,  1.04s/it]\n",
      "\t\tOverlap=0.55: 100%|██████████| 81/81 [01:23<00:00,  1.03s/it]\n",
      "\t\tOverlap=0.56: 100%|██████████| 81/81 [01:25<00:00,  1.05s/it]\n",
      "\t\tOverlap=0.57: 100%|██████████| 81/81 [01:24<00:00,  1.04s/it]\n",
      "\t\tOverlap=0.58: 100%|██████████| 81/81 [01:24<00:00,  1.04s/it]\n",
      "\t\tOverlap=0.59: 100%|██████████| 81/81 [01:25<00:00,  1.05s/it]\n",
      "\t\tOverlap=0.6: 100%|██████████| 81/81 [01:24<00:00,  1.04s/it]\n",
      "\t\tOverlap=0.61: 100%|██████████| 81/81 [01:24<00:00,  1.04s/it]\n",
      "\t\tOverlap=0.62: 100%|██████████| 81/81 [01:23<00:00,  1.03s/it]\n",
      "\t\tOverlap=0.63: 100%|██████████| 81/81 [01:24<00:00,  1.04s/it]\n",
      "\t\tOverlap=0.64: 100%|██████████| 81/81 [01:23<00:00,  1.03s/it]\n",
      "\t\tOverlap=0.65: 100%|██████████| 81/81 [01:24<00:00,  1.04s/it]\n",
      "\t\tOverlap=0.66: 100%|██████████| 81/81 [01:24<00:00,  1.04s/it]\n",
      "\t\tOverlap=0.67: 100%|██████████| 81/81 [01:23<00:00,  1.04s/it]\n",
      "\t\tOverlap=0.68: 100%|██████████| 81/81 [01:25<00:00,  1.05s/it]\n",
      "\t\tOverlap=0.69: 100%|██████████| 81/81 [01:24<00:00,  1.04s/it]\n",
      "\t\tOverlap=0.7: 100%|██████████| 81/81 [01:23<00:00,  1.03s/it]\n",
      "\t\tOverlap=0.71: 100%|██████████| 81/81 [01:24<00:00,  1.04s/it]\n",
      "\t\tOverlap=0.72: 100%|██████████| 81/81 [01:24<00:00,  1.04s/it]\n",
      "\t\tOverlap=0.73: 100%|██████████| 81/81 [01:24<00:00,  1.04s/it]\n",
      "\t\tOverlap=0.74: 100%|██████████| 81/81 [01:25<00:00,  1.05s/it]\n",
      "\t\tOverlap=0.75: 100%|██████████| 81/81 [01:23<00:00,  1.03s/it]\n",
      "\t\tOverlap=0.76: 100%|██████████| 81/81 [01:25<00:00,  1.05s/it]\n",
      "\t\tOverlap=0.77: 100%|██████████| 81/81 [01:23<00:00,  1.03s/it]\n",
      "\t\tOverlap=0.78: 100%|██████████| 81/81 [01:23<00:00,  1.03s/it]\n",
      "\t\tOverlap=0.79: 100%|██████████| 81/81 [01:24<00:00,  1.05s/it]\n",
      "\t\tOverlap=0.8: 100%|██████████| 81/81 [01:24<00:00,  1.04s/it]\n",
      "\t\tOverlap=0.81: 100%|██████████| 81/81 [01:23<00:00,  1.03s/it]\n",
      "\t\tOverlap=0.82: 100%|██████████| 81/81 [01:24<00:00,  1.04s/it]\n",
      "\t\tOverlap=0.83: 100%|██████████| 81/81 [01:24<00:00,  1.04s/it]\n",
      "\t\tOverlap=0.84: 100%|██████████| 81/81 [01:25<00:00,  1.06s/it]\n",
      "\t\tOverlap=0.85: 100%|██████████| 81/81 [01:25<00:00,  1.05s/it]\n",
      "\t\tOverlap=0.86: 100%|██████████| 81/81 [01:22<00:00,  1.02s/it]\n",
      "\t\tOverlap=0.87: 100%|██████████| 81/81 [01:24<00:00,  1.04s/it]\n",
      "\t\tOverlap=0.88: 100%|██████████| 81/81 [01:25<00:00,  1.05s/it]\n",
      "\t\tOverlap=0.89: 100%|██████████| 81/81 [01:24<00:00,  1.05s/it]\n",
      "\t\tOverlap=0.9: 100%|██████████| 81/81 [01:26<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tProcessing BA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tOverlap=0.1: 100%|██████████| 81/81 [01:20<00:00,  1.01it/s]\n",
      "\t\tOverlap=0.11: 100%|██████████| 81/81 [01:20<00:00,  1.01it/s]\n",
      "\t\tOverlap=0.12: 100%|██████████| 81/81 [01:20<00:00,  1.01it/s]\n",
      "\t\tOverlap=0.13: 100%|██████████| 81/81 [01:19<00:00,  1.02it/s]\n",
      "\t\tOverlap=0.14: 100%|██████████| 81/81 [01:19<00:00,  1.02it/s]\n",
      "\t\tOverlap=0.15: 100%|██████████| 81/81 [01:19<00:00,  1.02it/s]\n",
      "\t\tOverlap=0.16: 100%|██████████| 81/81 [01:19<00:00,  1.02it/s]\n",
      "\t\tOverlap=0.17: 100%|██████████| 81/81 [01:19<00:00,  1.02it/s]\n",
      "\t\tOverlap=0.18: 100%|██████████| 81/81 [01:18<00:00,  1.03it/s]\n",
      "\t\tOverlap=0.19: 100%|██████████| 81/81 [01:17<00:00,  1.04it/s]\n",
      "\t\tOverlap=0.2: 100%|██████████| 81/81 [01:18<00:00,  1.03it/s]\n",
      "\t\tOverlap=0.21: 100%|██████████| 81/81 [01:18<00:00,  1.03it/s]\n",
      "\t\tOverlap=0.22: 100%|██████████| 81/81 [01:18<00:00,  1.03it/s]\n",
      "\t\tOverlap=0.23: 100%|██████████| 81/81 [01:18<00:00,  1.04it/s]\n",
      "\t\tOverlap=0.24: 100%|██████████| 81/81 [01:39<00:00,  1.23s/it]\n",
      "\t\tOverlap=0.25: 100%|██████████| 81/81 [01:39<00:00,  1.22s/it]\n",
      "\t\tOverlap=0.26: 100%|██████████| 81/81 [01:37<00:00,  1.20s/it]\n",
      "\t\tOverlap=0.27: 100%|██████████| 81/81 [01:40<00:00,  1.24s/it]\n",
      "\t\tOverlap=0.28: 100%|██████████| 81/81 [01:16<00:00,  1.05it/s]\n",
      "\t\tOverlap=0.29: 100%|██████████| 81/81 [01:16<00:00,  1.05it/s]\n",
      "\t\tOverlap=0.3: 100%|██████████| 81/81 [01:17<00:00,  1.05it/s]\n",
      "\t\tOverlap=0.31: 100%|██████████| 81/81 [01:17<00:00,  1.05it/s]\n",
      "\t\tOverlap=0.32: 100%|██████████| 81/81 [01:17<00:00,  1.05it/s]\n",
      "\t\tOverlap=0.33: 100%|██████████| 81/81 [01:49<00:00,  1.35s/it]\n",
      "\t\tOverlap=0.34: 100%|██████████| 81/81 [01:16<00:00,  1.05it/s]\n",
      "\t\tOverlap=0.35: 100%|██████████| 81/81 [01:16<00:00,  1.05it/s]\n",
      "\t\tOverlap=0.36: 100%|██████████| 81/81 [01:16<00:00,  1.06it/s]\n",
      "\t\tOverlap=0.37: 100%|██████████| 81/81 [01:16<00:00,  1.05it/s]\n",
      "\t\tOverlap=0.38: 100%|██████████| 81/81 [01:16<00:00,  1.06it/s]\n",
      "\t\tOverlap=0.39: 100%|██████████| 81/81 [01:59<00:00,  1.48s/it]\n",
      "\t\tOverlap=0.4: 100%|██████████| 81/81 [01:16<00:00,  1.05it/s]\n",
      "\t\tOverlap=0.41: 100%|██████████| 81/81 [01:16<00:00,  1.06it/s]\n",
      "\t\tOverlap=0.42: 100%|██████████| 81/81 [01:17<00:00,  1.05it/s]\n",
      "\t\tOverlap=0.43: 100%|██████████| 81/81 [01:17<00:00,  1.05it/s]\n",
      "\t\tOverlap=0.44: 100%|██████████| 81/81 [01:16<00:00,  1.06it/s]\n",
      "\t\tOverlap=0.45: 100%|██████████| 81/81 [01:16<00:00,  1.05it/s]\n",
      "\t\tOverlap=0.46: 100%|██████████| 81/81 [01:16<00:00,  1.06it/s]\n",
      "\t\tOverlap=0.47: 100%|██████████| 81/81 [01:16<00:00,  1.06it/s]\n",
      "\t\tOverlap=0.48: 100%|██████████| 81/81 [01:16<00:00,  1.05it/s]\n",
      "\t\tOverlap=0.49: 100%|██████████| 81/81 [01:16<00:00,  1.06it/s]\n",
      "\t\tOverlap=0.5: 100%|██████████| 81/81 [01:16<00:00,  1.06it/s]\n",
      "\t\tOverlap=0.51: 100%|██████████| 81/81 [01:16<00:00,  1.06it/s]\n",
      "\t\tOverlap=0.52: 100%|██████████| 81/81 [01:16<00:00,  1.05it/s]\n",
      "\t\tOverlap=0.53: 100%|██████████| 81/81 [01:16<00:00,  1.05it/s]\n",
      "\t\tOverlap=0.54: 100%|██████████| 81/81 [01:17<00:00,  1.05it/s]\n",
      "\t\tOverlap=0.55: 100%|██████████| 81/81 [01:16<00:00,  1.06it/s]\n",
      "\t\tOverlap=0.56: 100%|██████████| 81/81 [01:17<00:00,  1.05it/s]\n",
      "\t\tOverlap=0.57: 100%|██████████| 81/81 [01:16<00:00,  1.05it/s]\n",
      "\t\tOverlap=0.58: 100%|██████████| 81/81 [01:16<00:00,  1.06it/s]\n",
      "\t\tOverlap=0.59: 100%|██████████| 81/81 [01:16<00:00,  1.06it/s]\n",
      "\t\tOverlap=0.6: 100%|██████████| 81/81 [01:16<00:00,  1.06it/s]\n",
      "\t\tOverlap=0.61: 100%|██████████| 81/81 [02:18<00:00,  1.71s/it]\n",
      "\t\tOverlap=0.62: 100%|██████████| 81/81 [01:16<00:00,  1.05it/s]\n",
      "\t\tOverlap=0.63: 100%|██████████| 81/81 [01:17<00:00,  1.05it/s]\n",
      "\t\tOverlap=0.64: 100%|██████████| 81/81 [02:23<00:00,  1.77s/it]\n",
      "\t\tOverlap=0.65: 100%|██████████| 81/81 [01:17<00:00,  1.04it/s]\n",
      "\t\tOverlap=0.66: 100%|██████████| 81/81 [02:41<00:00,  2.00s/it]\n",
      "\t\tOverlap=0.67: 100%|██████████| 81/81 [01:17<00:00,  1.05it/s]\n",
      "\t\tOverlap=0.68: 100%|██████████| 81/81 [01:17<00:00,  1.04it/s]\n",
      "\t\tOverlap=0.69: 100%|██████████| 81/81 [01:17<00:00,  1.05it/s]\n",
      "\t\tOverlap=0.7: 100%|██████████| 81/81 [01:17<00:00,  1.04it/s]\n",
      "\t\tOverlap=0.71: 100%|██████████| 81/81 [01:18<00:00,  1.03it/s]\n",
      "\t\tOverlap=0.72: 100%|██████████| 81/81 [01:17<00:00,  1.04it/s]\n",
      "\t\tOverlap=0.73: 100%|██████████| 81/81 [01:18<00:00,  1.03it/s]\n",
      "\t\tOverlap=0.74: 100%|██████████| 81/81 [01:17<00:00,  1.04it/s]\n",
      "\t\tOverlap=0.75: 100%|██████████| 81/81 [01:18<00:00,  1.04it/s]\n",
      "\t\tOverlap=0.76: 100%|██████████| 81/81 [01:18<00:00,  1.04it/s]\n",
      "\t\tOverlap=0.77: 100%|██████████| 81/81 [01:17<00:00,  1.04it/s]\n",
      "\t\tOverlap=0.78: 100%|██████████| 81/81 [01:18<00:00,  1.04it/s]\n",
      "\t\tOverlap=0.79: 100%|██████████| 81/81 [01:19<00:00,  1.02it/s]\n",
      "\t\tOverlap=0.8: 100%|██████████| 81/81 [01:19<00:00,  1.02it/s]\n",
      "\t\tOverlap=0.81: 100%|██████████| 81/81 [01:18<00:00,  1.03it/s]\n",
      "\t\tOverlap=0.82: 100%|██████████| 81/81 [02:43<00:00,  2.02s/it]\n",
      "\t\tOverlap=0.83: 100%|██████████| 81/81 [02:31<00:00,  1.87s/it]\n",
      "\t\tOverlap=0.84: 100%|██████████| 81/81 [01:18<00:00,  1.03it/s]\n",
      "\t\tOverlap=0.85: 100%|██████████| 81/81 [01:19<00:00,  1.01it/s]\n",
      "\t\tOverlap=0.86: 100%|██████████| 81/81 [01:19<00:00,  1.02it/s]\n",
      "\t\tOverlap=0.87: 100%|██████████| 81/81 [01:19<00:00,  1.02it/s]\n",
      "\t\tOverlap=0.88: 100%|██████████| 81/81 [01:20<00:00,  1.00it/s]\n",
      "\t\tOverlap=0.89: 100%|██████████| 81/81 [02:29<00:00,  1.84s/it]\n",
      "\t\tOverlap=0.9: 100%|██████████| 81/81 [01:19<00:00,  1.01it/s]\n"
     ]
    }
   ],
   "source": [
    "def optimization_proportion(\n",
    "    k_range={\"start\": 2, \"end\": 10, \"step\": 0.1},\n",
    "    n_list=config.NETWORK_NODES_LIST,\n",
    "    overlap_list=config.NETWORK_OVERLAP,\n",
    "    result_columns=[\"network_type\", \"N\", \"<k>\", \"overlap\", \"MDS_1\", \"MDS_2\", \"Diff_MDS_1\", \"Diff_MDS_2\", \"UMDS_0\", \"UMDS_1\", \"UMDS_2\", \"average_depth\", \"time_1\", \"time_2\"]\n",
    "):\n",
    "    output_file_name = create_output_file(result_columns, \"optimization_proportion\")\n",
    "    \n",
    "    for n in n_list:\n",
    "        print(f\"Processing n={n}...\")\n",
    "        for network_type in [\"ER\", \"BA\"]:\n",
    "            print(f\"\\tProcessing {network_type}...\")\n",
    "            for overlap in overlap_list:\n",
    "                overlap = round(overlap, 3)\n",
    "                if overlap < 0:\n",
    "                    continue\n",
    "                for k in tqdm(np.arange(k_range['start'], k_range['end'] + k_range[\"step\"], k_range[\"step\"]), desc=f\"\\t\\tOverlap={overlap}\"):\n",
    "                    k = round(k, 2)\n",
    "                    dir = f\"{network_type}_n={n}_k={k}\"\n",
    "\n",
    "                    matchings = []\n",
    "                    for file in os.listdir(os.path.join(config.SYNTHETIC_NET_PATH, network_type, dir)):\n",
    "                        if \"base\" in file or f\"overlap={overlap}.txt\" in file:\n",
    "                            graph = read_network(os.path.join(config.SYNTHETIC_NET_PATH, network_type, dir, file), n)\n",
    "                            matching = Matching(graph)\n",
    "                            matching.HK_algorithm()\n",
    "                            matching.find_all_alternating_reachable_set()\n",
    "                            matchings.append(matching)\n",
    "                    \n",
    "                    multi_matching = MultiMatching(matchings)\n",
    "                    baseline_matching = copy.deepcopy(multi_matching)\n",
    "\n",
    "                    start_time = time.time()\n",
    "                    pre_diff_mds_1_size, pre_diff_mds_2_size, pre_union_size, union_size, average_depth = multi_matching.MOUI()\n",
    "                    end_time = time.time()\n",
    "                    time_2 = end_time - start_time\n",
    "                    \n",
    "                    start_time = time.time()\n",
    "                    min_union_size = baseline_matching.RRMU()\n",
    "                    end_time = time.time()\n",
    "                    time_1 = end_time - start_time\n",
    "\n",
    "                    with open(output_file_name, \"a\", encoding=\"utf-8\") as output_file:\n",
    "                        output_file.write(\",\".join([\n",
    "                            f\"{network_type}+{network_type}\", str(n), str(k), str(overlap),\n",
    "                            str(len(matchings[0].driver_nodes)), str(len(matchings[1].driver_nodes)), \n",
    "                            str(pre_diff_mds_1_size), str(pre_diff_mds_2_size), \n",
    "                            str(pre_union_size), str(min_union_size), str(union_size), str(average_depth),\n",
    "                            str(time_1), str(time_2)\n",
    "                        ]) + \"\\n\")\n",
    "\n",
    "optimization_proportion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homogeneous Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/51 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:46<00:00,  1.09it/s]\n",
      "100%|██████████| 51/51 [00:40<00:00,  1.26it/s]\n",
      "100%|██████████| 31/31 [00:21<00:00,  1.43it/s]\n"
     ]
    }
   ],
   "source": [
    "def same_structure(\n",
    "        k_range={\"start\": 1, \"end\": 6, \"step\": 0.1},\n",
    "        gamma_range={\"start\": 2, \"end\": 5, \"step\": 0.1},\n",
    "        result_columns=[\"type\", \"n\", \"k\", \"gamma\", \"seq\", \"MDS1\", \"MDS2\", \"pre_intersection\", \"intersection\", \"pre_union\", \"union\"]):\n",
    "    output_file_name = create_output_file(result_columns, \"same_structure\")\n",
    "\n",
    "    n = 1000\n",
    "\n",
    "    type = \"ER\"\n",
    "    for k in tqdm(np.arange(k_range['start'], k_range['end'] + k_range[\"step\"], k_range[\"step\"])):\n",
    "        k = round(k, 2)\n",
    "        for seq in range(10):\n",
    "            dir = f\"{type}_n={n}_k={k}\"\n",
    "            matchings = []\n",
    "            for file in os.listdir(os.path.join(config.SYNTHETIC_NET_PATH, type, dir)):\n",
    "                graph = read_network(os.path.join(config.SYNTHETIC_NET_PATH, type, dir, file), n)\n",
    "                matching = Matching(graph)\n",
    "                matching.HK_algorithm()\n",
    "                matching.find_all_alternating_reachable_set()\n",
    "                matchings.append(matching)\n",
    "            \n",
    "            multi_matching = MultiMatching(matchings)\n",
    "            pre_intersection_size, intersection_size, pre_union_size, union_size = multi_matching.MOUI()\n",
    "            with open(output_file_name, \"a\", encoding=\"utf-8\") as output_file:\n",
    "                output_file.write(\",\".join([\n",
    "                    f\"{type}+{type}\", str(n), str(k), '', str(seq), \n",
    "                    str(len(matchings[0].driver_nodes)), str(len(matchings[1].driver_nodes)), \n",
    "                    str(pre_intersection_size), str(intersection_size), str(pre_union_size), str(union_size)\n",
    "                ]) + \"\\n\")\n",
    "    \n",
    "    type = \"SF\"\n",
    "    gamma = round(3.0, 2)\n",
    "    for k in tqdm(np.arange(k_range['start'], k_range['end'] + k_range[\"step\"], k_range[\"step\"])):\n",
    "        k = round(k, 2)\n",
    "        for seq in range(10):\n",
    "            dir = f\"{type}_n={n}_k={k}_gamma={gamma}\"\n",
    "            matchings = []\n",
    "            for file in os.listdir(os.path.join(config.SYNTHETIC_NET_PATH, type, dir)):\n",
    "                graph = read_network(os.path.join(config.SYNTHETIC_NET_PATH, type, dir, file), n)\n",
    "                matching = Matching(graph)\n",
    "                matching.HK_algorithm()\n",
    "                matching.find_all_alternating_reachable_set()\n",
    "                matchings.append(matching)\n",
    "            \n",
    "            multi_matching = MultiMatching(matchings)\n",
    "            pre_intersection_size, intersection_size, pre_union_size, union_size = multi_matching.MOUI()\n",
    "            with open(output_file_name, \"a\", encoding=\"utf-8\") as output_file:\n",
    "                output_file.write(\",\".join([\n",
    "                    f\"{type}+{type}_gamma={gamma}\", str(n), str(k), str(gamma), str(seq), \n",
    "                    str(len(matchings[0].driver_nodes)), str(len(matchings[1].driver_nodes)), \n",
    "                    str(pre_intersection_size), str(intersection_size), str(pre_union_size), str(union_size)\n",
    "                ]) + \"\\n\")\n",
    "\n",
    "    k = round(3.0, 2)\n",
    "    for gamma in tqdm(np.arange(gamma_range['start'], gamma_range['end'] + gamma_range[\"step\"], gamma_range[\"step\"])):\n",
    "        gamma = round(gamma, 2)\n",
    "        for seq in range(10):\n",
    "            dir = f\"{type}_n={n}_k={k}_gamma={gamma}\"\n",
    "            matchings = []\n",
    "            for file in os.listdir(os.path.join(config.SYNTHETIC_NET_PATH, type, dir)):\n",
    "                graph = read_network(os.path.join(config.SYNTHETIC_NET_PATH, type, dir, file), n)\n",
    "                matching = Matching(graph)\n",
    "                matching.HK_algorithm()\n",
    "                matching.find_all_alternating_reachable_set()\n",
    "                matchings.append(matching)\n",
    "            \n",
    "            multi_matching = MultiMatching(matchings)\n",
    "            pre_intersection_size, intersection_size, pre_union_size, union_size = multi_matching.MOUI()\n",
    "            with open(output_file_name, \"a\", encoding=\"utf-8\") as output_file:\n",
    "                output_file.write(\",\".join([\n",
    "                    f\"{type}+{type}_k={k}\", str(n), str(k), str(gamma), str(seq), \n",
    "                    str(len(matchings[0].driver_nodes)), str(len(matchings[1].driver_nodes)), \n",
    "                    str(pre_intersection_size), str(intersection_size), str(pre_union_size), str(union_size)\n",
    "                ]) + \"\\n\")\n",
    "\n",
    "same_structure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heterogeneous Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 20/51 [7:12:35<11:10:31, 1297.80s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 97\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(output_file_name, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m output_file:\n\u001b[1;32m     91\u001b[0m                 output_file\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\n\u001b[1;32m     92\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m+\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mstr\u001b[39m(n), \u001b[38;5;28mstr\u001b[39m(k1), \u001b[38;5;28mstr\u001b[39m(k2), \u001b[38;5;28mstr\u001b[39m(gamma), \u001b[38;5;28mstr\u001b[39m(gamma), \n\u001b[1;32m     93\u001b[0m                     \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mlen\u001b[39m(matching1\u001b[38;5;241m.\u001b[39mdriver_nodes)), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mlen\u001b[39m(matching2\u001b[38;5;241m.\u001b[39mdriver_nodes)), \n\u001b[1;32m     94\u001b[0m                     \u001b[38;5;28mstr\u001b[39m(pre_intersection_size), \u001b[38;5;28mstr\u001b[39m(intersection_size), \u001b[38;5;28mstr\u001b[39m(pre_union_size), \u001b[38;5;28mstr\u001b[39m(union_size)\n\u001b[1;32m     95\u001b[0m                 ]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 97\u001b[0m \u001b[43msame_structure_diff_k_or_gamma\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 30\u001b[0m, in \u001b[0;36msame_structure_diff_k_or_gamma\u001b[0;34m(k_range, gamma_range, result_columns)\u001b[0m\n\u001b[1;32m     27\u001b[0m matchings\u001b[38;5;241m.\u001b[39mappend(matching2)\n\u001b[1;32m     29\u001b[0m multi_matching \u001b[38;5;241m=\u001b[39m MultiMatching(matchings)\n\u001b[0;32m---> 30\u001b[0m pre_intersection_size, intersection_size, pre_union_size, union_size \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_matching\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_UMDS\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(output_file_name, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m output_file:\n\u001b[1;32m     32\u001b[0m     output_file\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m+\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mstr\u001b[39m(n), \u001b[38;5;28mstr\u001b[39m(k1), \u001b[38;5;28mstr\u001b[39m(k2), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     34\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mlen\u001b[39m(matching1\u001b[38;5;241m.\u001b[39mdriver_nodes)), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mlen\u001b[39m(matching2\u001b[38;5;241m.\u001b[39mdriver_nodes)), \n\u001b[1;32m     35\u001b[0m         \u001b[38;5;28mstr\u001b[39m(pre_intersection_size), \u001b[38;5;28mstr\u001b[39m(intersection_size), \u001b[38;5;28mstr\u001b[39m(pre_union_size), \u001b[38;5;28mstr\u001b[39m(union_size)\n\u001b[1;32m     36\u001b[0m     ]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/ZhengHaoyu/python/1_important/OCMN/matching.py:373\u001b[0m, in \u001b[0;36mMultiMatching.find_UMDS\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    370\u001b[0m matcher_2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmatchings[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    372\u001b[0m diff_mds_1 \u001b[38;5;241m=\u001b[39m matcher_1\u001b[38;5;241m.\u001b[39mdriver_nodes \u001b[38;5;241m-\u001b[39m (matcher_1\u001b[38;5;241m.\u001b[39mdriver_nodes \u001b[38;5;241m&\u001b[39m matcher_2\u001b[38;5;241m.\u001b[39mdriver_nodes)\n\u001b[0;32m--> 373\u001b[0m diff_mds_2 \u001b[38;5;241m=\u001b[39m matcher_2\u001b[38;5;241m.\u001b[39mdriver_nodes \u001b[38;5;241m-\u001b[39m (matcher_1\u001b[38;5;241m.\u001b[39mdriver_nodes \u001b[38;5;241m&\u001b[39m matcher_2\u001b[38;5;241m.\u001b[39mdriver_nodes)\n\u001b[1;32m    375\u001b[0m pre_union_size, pre_intersection_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_info(\n\u001b[1;32m    376\u001b[0m     [matcher_1\u001b[38;5;241m.\u001b[39mdriver_nodes, matcher_2\u001b[38;5;241m.\u001b[39mdriver_nodes], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    377\u001b[0m )\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m driver_node \u001b[38;5;129;01min\u001b[39;00m diff_mds_1\u001b[38;5;241m.\u001b[39mcopy():\n",
      "File \u001b[0;32m~/Documents/ZhengHaoyu/python/1_important/OCMN/matching.py:353\u001b[0m, in \u001b[0;36mMultiMatching.find_UMDS.<locals>.traverse\u001b[0;34m(current_layer, hierarchy_nodes_num, traverse_queue)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m intermediate_node \u001b[38;5;129;01min\u001b[39;00m intermediate_node_set:\n\u001b[1;32m    352\u001b[0m     new_exchange_history \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(exchange_pairs)\n\u001b[0;32m--> 353\u001b[0m     new_exchange_history\u001b[38;5;241m.\u001b[39mappend((current_layer, (current_node, intermediate_node)))\n\u001b[1;32m    354\u001b[0m     traverse_queue\u001b[38;5;241m.\u001b[39mappend((intermediate_node, new_exchange_history))\n\u001b[1;32m    355\u001b[0m     next_hierarchy_nodes_num \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Documents/ZhengHaoyu/python/1_important/OCMN/matching.py:353\u001b[0m, in \u001b[0;36mMultiMatching.find_UMDS.<locals>.traverse\u001b[0;34m(current_layer, hierarchy_nodes_num, traverse_queue)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m intermediate_node \u001b[38;5;129;01min\u001b[39;00m intermediate_node_set:\n\u001b[1;32m    352\u001b[0m     new_exchange_history \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(exchange_pairs)\n\u001b[0;32m--> 353\u001b[0m     new_exchange_history\u001b[38;5;241m.\u001b[39mappend((current_layer, (current_node, intermediate_node)))\n\u001b[1;32m    354\u001b[0m     traverse_queue\u001b[38;5;241m.\u001b[39mappend((intermediate_node, new_exchange_history))\n\u001b[1;32m    355\u001b[0m     next_hierarchy_nodes_num \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "    \u001b[0;31m[... skipping similar frames: MultiMatching.find_UMDS.<locals>.traverse at line 353 (7 times)]\u001b[0m\n",
      "File \u001b[0;32m~/Documents/ZhengHaoyu/python/1_important/OCMN/matching.py:353\u001b[0m, in \u001b[0;36mMultiMatching.find_UMDS.<locals>.traverse\u001b[0;34m(current_layer, hierarchy_nodes_num, traverse_queue)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m intermediate_node \u001b[38;5;129;01min\u001b[39;00m intermediate_node_set:\n\u001b[1;32m    352\u001b[0m     new_exchange_history \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(exchange_pairs)\n\u001b[0;32m--> 353\u001b[0m     new_exchange_history\u001b[38;5;241m.\u001b[39mappend((current_layer, (current_node, intermediate_node)))\n\u001b[1;32m    354\u001b[0m     traverse_queue\u001b[38;5;241m.\u001b[39mappend((intermediate_node, new_exchange_history))\n\u001b[1;32m    355\u001b[0m     next_hierarchy_nodes_num \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Documents/ZhengHaoyu/python/1_important/OCMN/matching.py:310\u001b[0m, in \u001b[0;36mMultiMatching.find_UMDS.<locals>.traverse\u001b[0;34m(current_layer, hierarchy_nodes_num, traverse_queue)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m current_layer \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayer\u001b[38;5;241m.\u001b[39mOne:\n\u001b[1;32m    309\u001b[0m     alternating_reachable_set \u001b[38;5;241m=\u001b[39m matcher_1\u001b[38;5;241m.\u001b[39mfind_alternating_reachable_set(current_node)\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m current_layer \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayer\u001b[38;5;241m.\u001b[39mTwo:\n\u001b[1;32m    311\u001b[0m     alternating_reachable_set \u001b[38;5;241m=\u001b[39m matcher_2\u001b[38;5;241m.\u001b[39mfind_reversal_alternating_reachable_set(current_node)\n\u001b[1;32m    312\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    - For node \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_node\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in layer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_layer\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(alternating_reachable_set)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m alternating reachable nodes.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/ZhengHaoyu/python/1_important/OCMN/matching.py:134\u001b[0m, in \u001b[0;36mMatching.find_reversal_alternating_reachable_set\u001b[0;34m(self, matched)\u001b[0m\n\u001b[1;32m    131\u001b[0m current_node \u001b[38;5;241m=\u001b[39m queue\u001b[38;5;241m.\u001b[39mpopleft()\n\u001b[1;32m    132\u001b[0m predecessor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmarkedDes\u001b[38;5;241m.\u001b[39mget(current_node)\n\u001b[0;32m--> 134\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m successor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuccessors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredecessor\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m successor \u001b[38;5;129;01min\u001b[39;00m visited_nodes \u001b[38;5;129;01mor\u001b[39;00m successor \u001b[38;5;241m==\u001b[39m current_node:\n\u001b[1;32m    136\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/Public/anaconda3/envs/network/lib/python3.9/site-packages/networkx/classes/digraph.py:899\u001b[0m, in \u001b[0;36mDiGraph.successors\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns an iterator over successor nodes of n.\u001b[39;00m\n\u001b[1;32m    876\u001b[0m \n\u001b[1;32m    877\u001b[0m \u001b[38;5;124;03mA successor of n is a node m such that there exists a directed\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    896\u001b[0m \u001b[38;5;124;03mneighbors() and successors() are the same.\u001b[39;00m\n\u001b[1;32m    897\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28miter\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_succ\u001b[49m[n])\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    901\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NetworkXError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe node \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not in the digraph.\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def same_structure_diff_k_or_gamma(\n",
    "        k_range={\"start\": 1, \"end\": 6, \"step\": 0.1},\n",
    "        gamma_range={\"start\": 2, \"end\": 5, \"step\": 0.1}, \n",
    "        result_columns=[\"type\", \"n\", \"k1\", \"k2\", \"gamma1\", \"gamma2\", \"MDS1\", \"MDS2\", \"pre_intersection\", \"intersection\", \"pre_union\", \"union\"]):\n",
    "    output_file_name = create_output_file(result_columns, \"same_structure_diff_k_or_gamma\")\n",
    "\n",
    "    n = 1000\n",
    "\n",
    "    type = \"ER\"\n",
    "    for k1 in tqdm(np.arange(k_range['start'], k_range['end'] + k_range[\"step\"], k_range[\"step\"])):\n",
    "        k1 = round(k1, 2)\n",
    "        dir1 = f\"{type}_n={n}_k={k1}\"\n",
    "        for k2 in np.arange(k_range['start'], k_range['end'] + k_range[\"step\"], k_range[\"step\"]):\n",
    "            k2 = round(k2, 2)\n",
    "            dir2 = f\"{type}_n={n}_k={k2}\"\n",
    "\n",
    "            matchings = []\n",
    "            graph1 = read_network(os.path.join(config.SYNTHETIC_NET_PATH, type, dir1, \"1.txt\"), n)\n",
    "            matching1 = Matching(graph1)\n",
    "            matching1.HK_algorithm()\n",
    "            matching1.find_all_alternating_reachable_set()\n",
    "            matchings.append(matching1)\n",
    "            graph2 = read_network(os.path.join(config.SYNTHETIC_NET_PATH, type, dir2, \"2.txt\"), n)\n",
    "            matching2 = Matching(graph2)\n",
    "            matching2.HK_algorithm()\n",
    "            matching2.find_all_alternating_reachable_set()\n",
    "            matchings.append(matching2)\n",
    "\n",
    "            multi_matching = MultiMatching(matchings)\n",
    "            pre_intersection_size, intersection_size, pre_union_size, union_size = multi_matching.MOUI()\n",
    "            with open(output_file_name, \"a\", encoding=\"utf-8\") as output_file:\n",
    "                output_file.write(\",\".join([\n",
    "                    f\"{type}+{type}\", str(n), str(k1), str(k2), '', '', \n",
    "                    str(len(matching1.driver_nodes)), str(len(matching2.driver_nodes)), \n",
    "                    str(pre_intersection_size), str(intersection_size), str(pre_union_size), str(union_size)\n",
    "                ]) + \"\\n\")\n",
    "        \n",
    "    type = \"SF\"\n",
    "    k = round(3.0, 2)\n",
    "    for gamma1 in tqdm(np.arange(gamma_range['start'], gamma_range['end'] + gamma_range[\"step\"], gamma_range[\"step\"])):\n",
    "        gamma1 = round(gamma1, 2)\n",
    "        dir1 = f\"{type}_n={n}_k={k}_gamma={gamma1}\"\n",
    "        for gamma2 in np.arange(gamma_range['start'], gamma_range['end'] + gamma_range[\"step\"], gamma_range[\"step\"]):\n",
    "            gamma2 = round(gamma2, 2)\n",
    "            dir2 = f\"{type}_n={n}_k={k}_gamma={gamma2}\"\n",
    "\n",
    "            matchings = []\n",
    "            graph1 = read_network(os.path.join(config.SYNTHETIC_NET_PATH, type, dir1, \"1.txt\"), n)\n",
    "            matching1 = Matching(graph1)\n",
    "            matching1.HK_algorithm()\n",
    "            matching1.find_all_alternating_reachable_set()\n",
    "            matchings.append(matching1)\n",
    "            graph2 = read_network(os.path.join(config.SYNTHETIC_NET_PATH, type, dir2, \"2.txt\"), n)\n",
    "            matching2 = Matching(graph2)\n",
    "            matching2.HK_algorithm()\n",
    "            matching2.find_all_alternating_reachable_set()\n",
    "            matchings.append(matching2)\n",
    "\n",
    "            multi_matching = MultiMatching(matchings)\n",
    "            pre_intersection_size, intersection_size, pre_union_size, union_size = multi_matching.MOUI()\n",
    "            with open(output_file_name, \"a\", encoding=\"utf-8\") as output_file:\n",
    "                output_file.write(\",\".join([\n",
    "                    f\"{type}+{type}\", str(n), str(k), str(k), str(gamma1), str(gamma2), \n",
    "                    str(len(matching1.driver_nodes)), str(len(matching2.driver_nodes)), \n",
    "                    str(pre_intersection_size), str(intersection_size), str(pre_union_size), str(union_size)\n",
    "                ]) + \"\\n\")\n",
    "                \n",
    "    gamma = round(3.0, 2)\n",
    "    for k1 in tqdm(np.arange(k_range['start'], k_range['end'] + k_range[\"step\"], k_range[\"step\"])):\n",
    "        k1 = round(k1, 2)\n",
    "        dir1 = f\"{type}_n={n}_k={k1}_gamma={gamma}\"\n",
    "        for k2 in np.arange(k_range['start'], k_range['end'] + k_range[\"step\"], k_range[\"step\"]):\n",
    "            k2 = round(k2, 2)\n",
    "            dir2 = f\"{type}_n={n}_k={k2}_gamma={gamma}\"\n",
    "\n",
    "            matchings = []\n",
    "            graph1 = read_network(os.path.join(config.SYNTHETIC_NET_PATH, type, dir1, \"1.txt\"), n)\n",
    "            matching1 = Matching(graph1)\n",
    "            matching1.HK_algorithm()\n",
    "            matching1.find_all_alternating_reachable_set()\n",
    "            matchings.append(matching1)\n",
    "            graph2 = read_network(os.path.join(config.SYNTHETIC_NET_PATH, type, dir2, \"2.txt\"), n)\n",
    "            matching2 = Matching(graph2)\n",
    "            matching2.HK_algorithm()\n",
    "            matching2.find_all_alternating_reachable_set()\n",
    "            matchings.append(matching2)\n",
    "\n",
    "            multi_matching = MultiMatching(matchings)\n",
    "            pre_intersection_size, intersection_size, pre_union_size, union_size = multi_matching.MOUI()\n",
    "            with open(output_file_name, \"a\", encoding=\"utf-8\") as output_file:\n",
    "                output_file.write(\",\".join([\n",
    "                    f\"{type}+{type}\", str(n), str(k1), str(k2), str(gamma), str(gamma), \n",
    "                    str(len(matching1.driver_nodes)), str(len(matching2.driver_nodes)), \n",
    "                    str(pre_intersection_size), str(intersection_size), str(pre_union_size), str(union_size)\n",
    "                ]) + \"\\n\")\n",
    "\n",
    "same_structure_diff_k_or_gamma()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [03:01<00:00,  3.56s/it]\n"
     ]
    }
   ],
   "source": [
    "def diff_structure_diff_k(\n",
    "        k_range={\"start\": 1, \"end\": 6, \"step\": 0.1},\n",
    "        result_columns=[\"type\", \"n\", \"k1\", \"k2\", \"gamma1\", \"gamma2\", \"MDS1\", \"MDS2\", \"pre_intersection\", \"intersection\", \"pre_union\", \"union\"]):\n",
    "    output_file_name = create_output_file(result_columns, \"diff_structure_diff_k\")\n",
    "\n",
    "    n = 1000\n",
    "    \n",
    "    gamma = round(3.0, 2)\n",
    "    for k1 in tqdm(np.arange(k_range['start'], k_range['end'] + k_range[\"step\"], k_range[\"step\"])):\n",
    "        k1 = round(k1, 2)\n",
    "        dir1 = f\"ER_n={n}_k={k1}\"\n",
    "        for k2 in np.arange(k_range['start'], k_range['end'] + k_range[\"step\"], k_range[\"step\"]):\n",
    "            k2 = round(k2, 2)\n",
    "            dir2 = f\"SF_n={n}_k={k2}_gamma={gamma}\"\n",
    "\n",
    "            matchings = []\n",
    "            graph1 = read_network(os.path.join(config.SYNTHETIC_NET_PATH, 'ER', dir1, \"1.txt\"), n)\n",
    "            matching1 = Matching(graph1)\n",
    "            matching1.HK_algorithm()\n",
    "            matching1.find_all_alternating_reachable_set()\n",
    "            matchings.append(matching1)\n",
    "            graph2 = read_network(os.path.join(config.SYNTHETIC_NET_PATH, 'SF', dir2, \"1.txt\"), n)\n",
    "            matching2 = Matching(graph2)\n",
    "            matching2.HK_algorithm()\n",
    "            matching2.find_all_alternating_reachable_set()\n",
    "            matchings.append(matching2)\n",
    "\n",
    "            multi_matching = MultiMatching(matchings)\n",
    "            pre_intersection_size, intersection_size, pre_union_size, union_size = multi_matching.MOUI()\n",
    "            with open(output_file_name, \"a\", encoding=\"utf-8\") as output_file:\n",
    "                output_file.write(\",\".join([\n",
    "                    f\"ER+SF\", str(n), str(k1), str(k2), '', str(gamma), \n",
    "                    str(len(matching1.driver_nodes)), str(len(matching2.driver_nodes)), \n",
    "                    str(pre_intersection_size), str(intersection_size), str(pre_union_size), str(union_size)\n",
    "                ]) + \"\\n\")\n",
    "\n",
    "diff_structure_diff_k()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [01:40<00:00,  1.98s/it]\n"
     ]
    }
   ],
   "source": [
    "def diff_structure_diff_k_or_gamma(\n",
    "        k_range={\"start\": 1, \"end\": 6, \"step\": 0.1},\n",
    "        gamma_range={\"start\": 2, \"end\": 5, \"step\": 0.1}, \n",
    "        result_columns=[\"type\", \"n\", \"k1\", \"k2\", \"gamma1\", \"gamma2\", \"MDS1\", \"MDS2\", \"pre_intersection\", \"intersection\", \"pre_union\", \"union\"]):\n",
    "    output_file_name = create_output_file(result_columns, \"diff_structure_diff_k_or_gamma\")\n",
    "\n",
    "    n = 1000\n",
    "\n",
    "    for k in tqdm(np.arange(k_range['start'], k_range['end'] + k_range[\"step\"], k_range[\"step\"])):\n",
    "        k = round(k, 2)\n",
    "        dir1 = f\"ER_n={n}_k={k}\"\n",
    "        for gamma in np.arange(gamma_range['start'], gamma_range['end'] + gamma_range[\"step\"], gamma_range[\"step\"]):\n",
    "            gamma = round(gamma, 2)\n",
    "            dir2 = f\"SF_n={n}_k={k}_gamma={gamma}\"\n",
    "\n",
    "            matchings = []\n",
    "            graph1 = read_network(os.path.join(config.SYNTHETIC_NET_PATH, 'ER', dir1, \"1.txt\"), n)\n",
    "            matching1 = Matching(graph1)\n",
    "            matching1.HK_algorithm()\n",
    "            matching1.find_all_alternating_reachable_set()\n",
    "            matchings.append(matching1)\n",
    "            graph2 = read_network(os.path.join(config.SYNTHETIC_NET_PATH, 'SF', dir2, \"1.txt\"), n)\n",
    "            matching2 = Matching(graph2)\n",
    "            matching2.HK_algorithm()\n",
    "            matching2.find_all_alternating_reachable_set()\n",
    "            matchings.append(matching2)\n",
    "\n",
    "            multi_matching = MultiMatching(matchings)\n",
    "            pre_intersection_size, intersection_size, pre_union_size, union_size = multi_matching.MOUI()\n",
    "            with open(output_file_name, \"a\", encoding=\"utf-8\") as output_file:\n",
    "                output_file.write(\",\".join([\n",
    "                    f\"ER+SF\", str(n), str(k), str(k), '', str(gamma), \n",
    "                    str(len(matching1.driver_nodes)), str(len(matching2.driver_nodes)), \n",
    "                    str(pre_intersection_size), str(intersection_size), str(pre_union_size), str(union_size)\n",
    "                ]) + \"\\n\")\n",
    "\n",
    "diff_structure_diff_k_or_gamma()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_n_type_k(\n",
    "        k_range={\"start\": 1, \"end\": 8, \"step\": 0.1}, \n",
    "        result_columns=[\"type\", \"n\", \"k\", \"seq\", \"MDS1\", \"MDS2\", \"pre_intersection\", \"intersection\", \"pre_union\", \"union\"]):\n",
    "    output_file_name = create_output_file(result_columns)\n",
    "    \n",
    "    for k in np.arange(k_range['start'], k_range['end'] + k_range[\"step\"], k_range[\"step\"]):\n",
    "        k = round(k, 2)\n",
    "        for n in config.NETWORK_NODES_LIST:\n",
    "            for seq in range(10):\n",
    "                for type, _ in config.NETWORK_TYPES.items():\n",
    "                    dir = f\"{type}_n={n}_k={k}\"\n",
    "                    print(dir)\n",
    "                    matchings = []\n",
    "                    for file in os.listdir(os.path.join(config.SYNTHETIC_NET_PATH, dir)):\n",
    "                        graph = read_network(os.path.join(config.SYNTHETIC_NET_PATH, dir, file), n)\n",
    "                        matching = Matching(graph)\n",
    "                        matching.HK_algorithm()\n",
    "                        matching.find_all_alternating_reachable_set()\n",
    "                        matchings.append(matching)\n",
    "                    \n",
    "                    multi_matching = MultiMatching(matchings)\n",
    "                    pre_intersection_size, intersection_size, pre_union_size, union_size = multi_matching.MOUI()\n",
    "                    with open(output_file_name, \"a\", encoding=\"utf-8\") as output_file:\n",
    "                        output_file.write(\",\".join([f\"{type}+{type}\", str(n), str(k), str(seq), str(len(matchings[0].driver_nodes)), str(len(matchings[1].driver_nodes)), \n",
    "                                                    str(pre_intersection_size), str(intersection_size), str(pre_union_size), str(union_size)]) + \"\\n\")\n",
    "                matchings = []\n",
    "                for type, _ in config.NETWORK_TYPES.items():\n",
    "                    dir = f\"{type}_n={n}_k={k}\"\n",
    "                    print(dir)\n",
    "                    graph = read_network(os.path.join(config.SYNTHETIC_NET_PATH, dir, \"1.txt\"), n)\n",
    "                    matching = Matching(graph)\n",
    "                    matching.HK_algorithm()\n",
    "                    matching.find_all_alternating_reachable_set()\n",
    "                    matchings.append(matching)\n",
    "                \n",
    "                multi_matching = MultiMatching(matchings)\n",
    "                pre_intersection_size, intersection_size, pre_union_size, union_size = multi_matching.MOUI()\n",
    "                with open(output_file_name, \"a\", encoding=\"utf-8\") as output_file:\n",
    "                    output_file.write(\",\".join([\n",
    "                        f\"ER+BA\", str(n), str(k), str(seq), \n",
    "                        str(len(matchings[0].driver_nodes)), str(len(matchings[1].driver_nodes)), \n",
    "                        str(pre_intersection_size), str(intersection_size), str(pre_union_size), str(union_size)\n",
    "                    ]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_n_gamma(\n",
    "        gamma_range={\"start\": 2, \"end\": 8, \"step\": 0.1}, \n",
    "        result_columns=[\"type\", \"n\", \"k\", \"gamma\", \"seq\", \"MDS1\", \"MDS2\", \"pre_intersection\", \"intersection\", \"pre_union\", \"union\"]):\n",
    "    output_file_name = create_output_file(result_columns)\n",
    "    \n",
    "    type = \"BA\"\n",
    "    k = round(3.0, 2)\n",
    "    for gamma in np.arange(gamma_range['start'], gamma_range['end'] + gamma_range[\"step\"], gamma_range[\"step\"]):\n",
    "        gamma = round(gamma, 2)\n",
    "        for n in config.NETWORK_NODES_LIST:\n",
    "            for seq in range(10):\n",
    "                dir = f\"{type}_n={n}_k={k}_gamma={gamma}\"\n",
    "                print(dir)\n",
    "                matchings = []\n",
    "                for file in os.listdir(os.path.join(config.SYNTHETIC_NET_PATH, 'BA', dir)):\n",
    "                    graph = read_network(os.path.join(config.SYNTHETIC_NET_PATH, 'BA', dir, file), n)\n",
    "                    matching = Matching(graph)\n",
    "                    matching.HK_algorithm()\n",
    "                    matching.find_all_alternating_reachable_set()\n",
    "                    matchings.append(matching)\n",
    "                \n",
    "                multi_matching = MultiMatching(matchings)\n",
    "                pre_intersection_size, intersection_size, pre_union_size, union_size = multi_matching.MOUI()\n",
    "                with open(output_file_name, \"a\", encoding=\"utf-8\") as output_file:\n",
    "                    output_file.write(\",\".join([\n",
    "                        f\"{type}+{type}\", str(n), str(k), str(gamma), str(seq), \n",
    "                        str(len(matchings[0].driver_nodes)), str(len(matchings[1].driver_nodes)), \n",
    "                        str(pre_intersection_size), str(intersection_size), str(pre_union_size), str(union_size)\n",
    "                    ]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_k(\n",
    "        k_range={\"start\": 1, \"end\": 8, \"step\": 0.1}, \n",
    "        result_columns=[\"type\", \"n\", \"k1\", \"k2\", \"MDS1\", \"MDS2\", \"pre_intersection\", \"intersection\", \"pre_union\", \"union\"]):\n",
    "    output_file_name = create_output_file(result_columns)\n",
    "\n",
    "    n = 1000\n",
    "    for type, _ in config.NETWORK_TYPES.items():\n",
    "        for k1 in np.arange(k_range['start'], k_range['end'] + k_range[\"step\"], k_range[\"step\"]):\n",
    "            k1 = round(k1, 2)\n",
    "            dir1 = f\"{type}_n={n}_k={k1}\"\n",
    "            for k2 in np.arange(k_range['start'], k_range['end'] + k_range[\"step\"], k_range[\"step\"]):\n",
    "                k2 = round(k2, 2)\n",
    "                dir2 = f\"{type}_n={n}_k={k2}\"\n",
    "\n",
    "                matchings = []\n",
    "                graph1 = read_network(os.path.join(config.SYNTHETIC_NET_PATH, dir1, \"1.txt\"), n)\n",
    "                matching1 = Matching(graph1)\n",
    "                matching1.HK_algorithm()\n",
    "                matching1.find_all_alternating_reachable_set()\n",
    "                matchings.append(matching1)\n",
    "                graph2 = read_network(os.path.join(config.SYNTHETIC_NET_PATH, dir2, \"2.txt\"), n)\n",
    "                matching2 = Matching(graph2)\n",
    "                matching2.HK_algorithm()\n",
    "                matching2.find_all_alternating_reachable_set()\n",
    "                matchings.append(matching2)\n",
    "\n",
    "                multi_matching = MultiMatching(matchings)\n",
    "                pre_intersection_size, intersection_size, pre_union_size, union_size = multi_matching.MOUI()\n",
    "                with open(output_file_name, \"a\", encoding=\"utf-8\") as output_file:\n",
    "                    output_file.write(\",\".join([\n",
    "                        f\"{type}+{type}\", str(n), str(k1), str(k2), \n",
    "                        str(len(matching1.driver_nodes)), str(len(matching2.driver_nodes)),\n",
    "                        str(pre_intersection_size), str(intersection_size), str(pre_union_size), str(union_size)\n",
    "                    ]) + \"\\n\")\n",
    "    \n",
    "    for k1 in np.arange(k_range['start'], k_range['end'] + k_range[\"step\"], k_range[\"step\"]):\n",
    "        k1 = round(k1, 2)\n",
    "        dir1 = f\"ER_n={n}_k={k1}\"\n",
    "        for k2 in np.arange(k_range['start'], k_range['end'] + k_range[\"step\"], k_range[\"step\"]):\n",
    "            k2 = round(k2, 2)\n",
    "            dir2 = f\"BA_n={n}_k={k2}\"\n",
    "\n",
    "            matchings = []\n",
    "            graph1 = read_network(os.path.join(config.SYNTHETIC_NET_PATH, dir1, \"1.txt\"), n)\n",
    "            matching1 = Matching(graph1)\n",
    "            matching1.HK_algorithm()\n",
    "            matching1.find_all_alternating_reachable_set()\n",
    "            matchings.append(matching1)\n",
    "            graph2 = read_network(os.path.join(config.SYNTHETIC_NET_PATH, dir2, \"1.txt\"), n)\n",
    "            matching2 = Matching(graph2)\n",
    "            matching2.HK_algorithm()\n",
    "            matching2.find_all_alternating_reachable_set()\n",
    "            matchings.append(matching2)\n",
    "\n",
    "            multi_matching = MultiMatching(matchings)\n",
    "            pre_intersection_size, intersection_size, pre_union_size, union_size = multi_matching.MOUI()\n",
    "            with open(output_file_name, \"a\", encoding=\"utf-8\") as output_file:\n",
    "                output_file.write(\",\".join([\n",
    "                    f\"ER+BA\", str(n), str(k1), str(k2), \n",
    "                    str(len(matching1.driver_nodes)), str(len(matching2.driver_nodes)),\n",
    "                    str(pre_intersection_size), str(intersection_size), str(pre_union_size), str(union_size)\n",
    "                ]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_gamma(\n",
    "        gamma_range={\"start\": 2, \"end\": 8, \"step\": 0.1}, \n",
    "        result_columns=[\"type\", \"n\", \"k\", \"gamma1\", \"gamma2\", \"MDS1\", \"MDS2\", \"pre_intersection\", \"intersection\", \"pre_union\", \"union\"]):\n",
    "    output_file_name = create_output_file(result_columns)\n",
    "\n",
    "    type = \"BA\"\n",
    "    n = 1000\n",
    "    k = round(3.0, 2)\n",
    "    for gamma1 in np.arange(gamma_range['start'], gamma_range['end'] + gamma_range[\"step\"], gamma_range[\"step\"]):\n",
    "        gamma1 = round(gamma1, 2)\n",
    "        dir1 = f\"{type}_n={n}_k={k}_gamma={gamma1}\"\n",
    "        for gamma2 in np.arange(gamma_range['start'], gamma_range['end'] + gamma_range[\"step\"], gamma_range[\"step\"]):\n",
    "            gamma2 = round(gamma2, 2)\n",
    "            dir2 = f\"{type}_n={n}_k={k}_gamma={gamma2}\"\n",
    "\n",
    "            matchings = []\n",
    "            graph1 = read_network(os.path.join(config.SYNTHETIC_NET_PATH, dir1, \"1.txt\"), n)\n",
    "            matching1 = Matching(graph1)\n",
    "            matching1.HK_algorithm()\n",
    "            matching1.find_all_alternating_reachable_set()\n",
    "            matchings.append(matching1)\n",
    "            graph2 = read_network(os.path.join(config.SYNTHETIC_NET_PATH, dir2, \"2.txt\"), n)\n",
    "            matching2 = Matching(graph2)\n",
    "            matching2.HK_algorithm()\n",
    "            matching2.find_all_alternating_reachable_set()\n",
    "            matchings.append(matching2)\n",
    "\n",
    "            multi_matching = MultiMatching(matchings)\n",
    "            pre_intersection_size, intersection_size, pre_union_size, union_size = multi_matching.MOUI()\n",
    "            with open(output_file_name, \"a\", encoding=\"utf-8\") as output_file:\n",
    "                output_file.write(\",\".join([\n",
    "                    f\"{type}+{type}\", str(n), str(k), str(gamma1), str(gamma2), \n",
    "                    str(len(matching1.driver_nodes)), str(len(matching2.driver_nodes)),\n",
    "                    str(pre_intersection_size), str(intersection_size), str(pre_union_size), str(union_size)\n",
    "                ]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def real_network(\n",
    "        result_columns=[\"Dataset\", \"N1\", \"N2\", \"E1\", \"E2\", \"MDS1\", \"MDS2\", \"t_HK1\", \"t_HK2\", \"t_ARS1\", \"t_ARS2\", \"t_MSS\", \"pre_intersection\", \"intersection\", \"pre_union\", \"union\"]):\n",
    "    output_file_name = create_output_file(result_columns)\n",
    "    base_path = 'assets/real_net2'\n",
    "    dataset_folders = [f.path for f in os.scandir(base_path) if f.is_dir()]\n",
    "    for folder in dataset_folders:\n",
    "        print(folder)\n",
    "\n",
    "        graph1 = nx.DiGraph()\n",
    "        graph2 = nx.DiGraph()\n",
    "        graphs = [graph1, graph2]\n",
    "\n",
    "        layers_files = glob.glob(os.path.join(folder, 'Dataset', '*_layers.txt'))\n",
    "        edges_files = glob.glob(os.path.join(folder, 'Dataset', '*_multiplex.edges'))\n",
    "        nodes_files = glob.glob(os.path.join(folder, 'Dataset', '*_nodes.txt'))\n",
    "\n",
    "        if not layers_files or not edges_files or not nodes_files:\n",
    "            print(f\"Missing files in dataset {folder}. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        # 选择第一个文件，如果有多个匹配的话\n",
    "        layers_file = layers_files[0]\n",
    "        edges_file = edges_files[0]\n",
    "        nodes_file = nodes_files[0]\n",
    "\n",
    "        layers = pd.read_csv(layers_file, sep=' ')\n",
    "\n",
    "        nodes = pd.read_csv(nodes_file, sep=' ', on_bad_lines='skip')\n",
    "\n",
    "        edges = pd.read_csv(edges_file, sep=' ', header=None)\n",
    "        edges.columns = ['layerID', 'node1', 'node2', 'weight']\n",
    "\n",
    "        # print(f\"layers: {layers}, {layers.columns}, {layers.iloc[0, 0]}, {layers.iloc[1, 0]}\")\n",
    "        # print(f\"nodes: {nodes}, {nodes.columns}\")\n",
    "        # print(f\"edges: {edges}\")\n",
    "        # break\n",
    "\n",
    "        for graph in graphs:\n",
    "            graph.add_nodes_from(nodes['nodeID'].tolist())\n",
    "\n",
    "        edges_layer1 = edges[edges['layerID'] == layers.iloc[0, 0]]\n",
    "        edges_layer2 = edges[edges['layerID'] == layers.iloc[1, 0]]\n",
    "\n",
    "        graph1.add_edges_from(edges_layer1[['node1', 'node2']].values.tolist())\n",
    "        graph2.add_edges_from(edges_layer2[['node1', 'node2']].values.tolist())\n",
    "        \n",
    "        matchings = []\n",
    "        matching1 = Matching(graph1)\n",
    "        _, hk_time1 = matching1.HK_algorithm()\n",
    "        _, ARS_time1 = matching1.find_all_alternating_reachable_set()\n",
    "        matchings.append(matching1)\n",
    "        matching2 = Matching(graph2)\n",
    "        _, hk_time2 = matching2.HK_algorithm()\n",
    "        _, ARS_time2 = matching2.find_all_alternating_reachable_set()\n",
    "        matchings.append(matching2)\n",
    "\n",
    "        multi_matching = MultiMatching(matchings)\n",
    "        [pre_intersection_size, intersection_size, pre_union_size, union_size], MSS_time = multi_matching.MOUI()\n",
    "        with open(output_file_name, \"a\", encoding=\"utf-8\") as output_file:\n",
    "            output_file.write(\",\".join([\n",
    "                folder.split(\"\\\\\")[-1],\n",
    "                str(graph1.number_of_nodes()), str(graph2.number_of_nodes()), \n",
    "                str(graph1.number_of_edges()), str(graph2.number_of_edges()), \n",
    "                str(len(matching1.driver_nodes)), str(len(matching2.driver_nodes)),\n",
    "                str(hk_time1), str(hk_time2), str(ARS_time1), str(ARS_time2), str(MSS_time),\n",
    "                str(pre_intersection_size), str(intersection_size), str(pre_union_size), str(union_size)\n",
    "            ]) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "network",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
